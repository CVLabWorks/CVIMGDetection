import torch
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import os
from datetime import datetime
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# ğŸ”¥ AMP æ”¯æŒ
from torch.amp import autocast, GradScaler


class Trainer:
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = config.DEVICE

        self.model.to(self.device)
        self.criterion = nn.CrossEntropyLoss()

        # ä¼˜åŒ–æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼ˆä½ çš„æ¨¡å‹ä¸­åˆ†ç±»å¤´æ˜¯å¯è®­ç»ƒçš„ï¼‰
        params = filter(lambda p: p.requires_grad, self.model.parameters())
        self.optimizer = torch.optim.AdamW(
            params,
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )

        self.scheduler = ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=config.SCHEDULER_FACTOR,
            patience=config.SCHEDULER_PATIENCE,
            min_lr=config.SCHEDULER_MIN_LR
        )

        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        log_dir = os.path.join(config.LOG_DIR, f"{config.MODEL_TYPE}_{timestamp}")
        self.writer = SummaryWriter(log_dir)
        print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")

        self.best_val_loss = float('inf')
        self.epochs_no_improve = 0
        os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)

        # ğŸ”¥ AMP åˆå§‹åŒ–
        self.use_amp = config.USE_AMP and (self.device.type == 'cuda')
        if self.use_amp:
            self.scaler = GradScaler(device='cuda')  # âœ… æ›´æ–°ä¸º torch.amp.GradScaler(device='cuda')
            print("âš¡ å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
        else:
            self.scaler = None
            if config.USE_AMP:
                print("âš ï¸ USE_AMP=Trueï¼Œä½†è®¾å¤‡é CUDAï¼ŒAMP å·²ç¦ç”¨")

    def train_epoch(self):
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(self.train_loader, desc="  â¤ è®­ç»ƒä¸­", leave=False)
        for x, y in pbar:
            x, y = x.to(self.device), y.to(self.device)
            self.optimizer.zero_grad()

            if self.use_amp:
                with autocast(device_type=self.device.type):  # âœ… æ›´æ–°ä¸º torch.amp.autocast(device_type='cuda')
                    outputs = self.model(x)
                    loss = self.criterion(outputs, y)
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(x)
                loss = self.criterion(outputs, y)
                loss.backward()
                self.optimizer.step()

            total_loss += loss.item()
            pred = outputs.argmax(-1)
            correct += (pred == y).sum().item()
            total += y.size(0)

        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100 * correct / total
        return avg_loss, accuracy

    def eval_epoch(self):
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for x, y in tqdm(self.val_loader, desc="  â¤ éªŒè¯ä¸­", leave=False):
                x, y = x.to(self.device), y.to(self.device)

                if self.use_amp:
                    with autocast(device_type=self.device.type):  # âœ… æ›´æ–°ä¸º torch.amp.autocast(device_type='cuda')
                        outputs = self.model(x)
                        loss = self.criterion(outputs, y)
                else:
                    outputs = self.model(x)
                    loss = self.criterion(outputs, y)

                total_loss += loss.item()
                pred = outputs.argmax(-1)
                all_preds.append(pred.cpu().numpy())
                all_labels.append(y.cpu().numpy())

        all_preds = np.concatenate(all_preds)
        all_labels = np.concatenate(all_labels)

        avg_loss = total_loss / len(self.val_loader)
        accuracy = accuracy_score(all_labels, all_preds)
        report = classification_report(
            all_labels, all_preds,
            target_names=['Not AI', 'AI'],
            output_dict=True,
            zero_division=0
        )

        metrics = {
            'loss': avg_loss,
            'accuracy': accuracy,
            'precision_not_ai': report['Not AI']['precision'],
            'recall_not_ai': report['Not AI']['recall'],
            'f1_not_ai': report['Not AI']['f1-score'],
            'precision_ai': report['AI']['precision'],
            'recall_ai': report['AI']['recall'],
            'f1_ai': report['AI']['f1-score'],
            'macro_f1': report['macro avg']['f1-score']
        }

        print("\n" + "=" * 60)
        print("éªŒè¯é›†åˆ†ç±»æŠ¥å‘Š:")
        print("=" * 60)
        print(classification_report(all_labels, all_preds, target_names=['Not AI', 'AI'], digits=4))
        print(f"æ€»ä½“å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
        print("=" * 60)

        return metrics

    def fit(self):
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...\n")

        for epoch in range(self.config.EPOCHS):
            print(f"{'=' * 20} Epoch {epoch + 1:2d} / {self.config.EPOCHS} {'=' * 20}")

            tr_loss, tr_acc = self.train_epoch()
            val_metrics = self.eval_epoch()

            self.scheduler.step(val_metrics['loss'])

            # TensorBoard
            self.writer.add_scalars("Loss", {"Train": tr_loss, "Val": val_metrics['loss']}, epoch)
            self.writer.add_scalars("Accuracy", {"Train": tr_acc / 100.0, "Val": val_metrics['accuracy']}, epoch)
            self.writer.add_scalars("F1-Score", {
                "Not_AI": val_metrics['f1_not_ai'],
                "AI": val_metrics['f1_ai'],
                "Macro": val_metrics['macro_f1']
            }, epoch)
            self.writer.add_scalar("Learning Rate", self.optimizer.param_groups[0]['lr'], epoch)

            print(f"ğŸ“ˆ Epoch {epoch + 1:2d} | "
                  f"Train Acc: {tr_acc:6.2f}% | "
                  f"Val Acc: {val_metrics['accuracy'] * 100:6.2f}% | "
                  f"Val Loss: {val_metrics['loss']:.5f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['loss'] < self.best_val_loss - self.config.EARLY_STOPPING_MIN_DELTA:
                self.best_val_loss = val_metrics['loss']
                self.epochs_no_improve = 0
                save_path = os.path.join(self.config.CHECKPOINT_DIR, "best_model.pth")
                torch.save(self.model.state_dict(), save_path)
                print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Val Loss: {val_metrics['loss']:.5f})")
            else:
                self.epochs_no_improve += 1
                print(f"âš ï¸ éªŒè¯æŸå¤±æœªæ˜¾è‘—ä¸‹é™ï¼ˆè¿ç»­ {self.epochs_no_improve}/{self.config.EARLY_STOPPING_PATIENCE} æ¬¡ï¼‰")

            if self.epochs_no_improve >= self.config.EARLY_STOPPING_PATIENCE:
                print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨ Epoch {epoch + 1} åœæ­¢è®­ç»ƒã€‚")
                break

        self.writer.close()
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½éªŒè¯æŸå¤±: {self.best_val_loss:.5f}")


class TransferLearningTrainer:
    def __init__(self, model, train_loader, val_loader, config, num_classes=None):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = config.DEVICE
        self.num_classes = num_classes if num_classes is not None else config.NUM_LABELS

        self.model.to(self.device)
        self.criterion = nn.CrossEntropyLoss()

        # ä¼˜åŒ–æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼ˆä½ çš„æ¨¡å‹ä¸­åˆ†ç±»å¤´æ˜¯å¯è®­ç»ƒçš„ï¼‰
        params = filter(lambda p: p.requires_grad, self.model.parameters())
        self.optimizer = torch.optim.AdamW(
            params,
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )

        self.scheduler = ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=config.SCHEDULER_FACTOR,
            patience=config.SCHEDULER_PATIENCE,
            min_lr=config.SCHEDULER_MIN_LR,
        )

        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        log_dir = os.path.join(config.LOG_DIR, f"transfer_{config.MODEL_TYPE}_{timestamp}")
        self.writer = SummaryWriter(log_dir)
        print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")

        self.best_val_loss = float('inf')
        self.epochs_no_improve = 0
        os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)

        # ğŸ”¥ AMP åˆå§‹åŒ–
        self.use_amp = config.USE_AMP and (self.device.type == 'cuda')
        if self.use_amp:
            self.scaler = GradScaler(device='cuda')  # âœ… æ›´æ–°ä¸º torch.amp.GradScaler(device='cuda')
            print("âš¡ å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
        else:
            self.scaler = None
            if config.USE_AMP:
                print("âš ï¸ USE_AMP=Trueï¼Œä½†è®¾å¤‡é CUDAï¼ŒAMP å·²ç¦ç”¨")

    def train_epoch(self):
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(self.train_loader, desc="  â¤ è®­ç»ƒä¸­", leave=False)
        for x, y in pbar:
            x, y = x.to(self.device), y.to(self.device)
            self.optimizer.zero_grad()

            if self.use_amp:
                with autocast(device_type=self.device.type):  # âœ… æ›´æ–°ä¸º torch.amp.autocast(device_type='cuda')
                    outputs = self.model(x)
                    loss = self.criterion(outputs, y)
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(x)
                loss = self.criterion(outputs, y)
                loss.backward()
                self.optimizer.step()

            total_loss += loss.item()
            pred = outputs.argmax(-1)
            correct += (pred == y).sum().item()
            total += y.size(0)

        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100 * correct / total
        return avg_loss, accuracy

    def eval_epoch(self):
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for x, y in tqdm(self.val_loader, desc="  â¤ éªŒè¯ä¸­", leave=False):
                x, y = x.to(self.device), y.to(self.device)

                if self.use_amp:
                    with autocast(device_type=self.device.type):  # âœ… æ›´æ–°ä¸º torch.amp.autocast(device_type='cuda')
                        outputs = self.model(x)
                        loss = self.criterion(outputs, y)
                else:
                    outputs = self.model(x)
                    loss = self.criterion(outputs, y)

                total_loss += loss.item()
                pred = outputs.argmax(-1)
                all_preds.append(pred.cpu().numpy())
                all_labels.append(y.cpu().numpy())

        all_preds = np.concatenate(all_preds)
        all_labels = np.concatenate(all_labels)

        avg_loss = total_loss / len(self.val_loader)
        accuracy = accuracy_score(all_labels, all_preds)

        # ä¸ºè½¬ç§»å­¦ä¹ åˆ›å»ºæ›´é€šç”¨çš„åˆ†ç±»æŠ¥å‘Š
        unique_labels = np.unique(all_labels)
        target_names = [f"Class_{i}" for i in unique_labels] if len(unique_labels) <= 10 else [f"Class_{i}" for i in
                                                                                               range(
                                                                                                   len(unique_labels))]

        try:
            report = classification_report(
                all_labels, all_preds,
                target_names=target_names,
                output_dict=True,
                zero_division=0
            )
        except:
            # å¦‚æœæ ‡ç­¾åç§°ä¸å®é™…æ ‡ç­¾ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤åç§°
            report = classification_report(
                all_labels, all_preds,
                output_dict=True,
                zero_division=0
            )

        metrics = {
            'loss': avg_loss,
            'accuracy': accuracy,
            'macro_f1': report['macro avg']['f1-score'] if 'macro avg' in report else 0
        }

        print("\n" + "=" * 60)
        print("éªŒè¯é›†åˆ†ç±»æŠ¥å‘Š:")
        print("=" * 60)
        print(classification_report(all_labels, all_preds, digits=4))
        print(f"æ€»ä½“å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
        print("=" * 60)

        return metrics

    def fit(self):
        print("ğŸš€ å¼€å§‹è¿ç§»å­¦ä¹ è®­ç»ƒ...\n")

        for epoch in range(self.config.EPOCHS):
            print(f"{'=' * 20} Epoch {epoch + 1:2d} / {self.config.EPOCHS} {'=' * 20}")

            tr_loss, tr_acc = self.train_epoch()
            val_metrics = self.eval_epoch()

            self.scheduler.step(val_metrics['loss'])

            # TensorBoard
            self.writer.add_scalars("Loss", {"Train": tr_loss, "Val": val_metrics['loss']}, epoch)
            self.writer.add_scalars("Accuracy", {"Train": tr_acc / 100.0, "Val": val_metrics['accuracy']}, epoch)
            self.writer.add_scalar("Learning Rate", self.optimizer.param_groups[0]['lr'], epoch)
            self.writer.add_scalar("F1-Score", val_metrics['macro_f1'], epoch)

            print(f"ğŸ“ˆ Epoch {epoch + 1:2d} | "
                  f"Train Acc: {tr_acc:6.2f}% | "
                  f"Val Acc: {val_metrics['accuracy'] * 100:6.2f}% | "
                  f"Val Loss: {val_metrics['loss']:.5f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['loss'] < self.best_val_loss - self.config.EARLY_STOPPING_MIN_DELTA:
                self.best_val_loss = val_metrics['loss']
                self.epochs_no_improve = 0
                save_path = os.path.join(self.config.CHECKPOINT_DIR, "transfer_best_model.pth")
                torch.save(self.model.state_dict(), save_path)
                print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Val Loss: {val_metrics['loss']:.5f})")
            else:
                self.epochs_no_improve += 1
                print(f"âš ï¸ éªŒè¯æŸå¤±æœªæ˜¾è‘—ä¸‹é™ï¼ˆè¿ç»­ {self.epochs_no_improve}/{self.config.EARLY_STOPPING_PATIENCE} æ¬¡ï¼‰")

            if self.epochs_no_improve >= self.config.EARLY_STOPPING_PATIENCE:
                print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨ Epoch {epoch + 1} åœæ­¢è®­ç»ƒã€‚")
                break

        self.writer.close()
        print(f"\nğŸ‰ è¿ç§»å­¦ä¹ è®­ç»ƒå®Œæˆï¼æœ€ä½éªŒè¯æŸå¤±: {self.best_val_loss:.5f}")